# -*- coding: utf-8 -*-
"""maze-optimal-path_dynamic-programming.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vrG41dO7xzjjkB8Otq54gALfHWksFwQj
"""

# !pip install ipython-autotime
# %load_ext autotime
# %unload_ext autotime

# from IPython.display import display, HTML
import numpy as np
import matplotlib.pyplot as plt
from matplotlib import colors
from matplotlib import animation, rc

class Maze:
    START = 'S'
    GOAL = 'G'
    CURRENT = '■'
    S_START = (3, 0)
    S_GOAL = (5, 8)
    V_START = 1
    V_GOAL = 2
    V_WALL = 3

    LEFT = '←'
    RIGHT = '→'
    UP = '↑'
    DOWN = '↓'

    FONTSIZE = 18

    REWARD = -1
    REWARD_GOAL = 50
    REWARD_NEXTGOAL = 25


    def __init__(self, width=9, height=6):
        self.shape = (height, width)
        self.countDraw = 0

        self.mkMaze()
        self.initSAP()
        self.mkPlt()

    def initSAP(self):
        gridStates = list(np.ndindex(self.shape))
        self.states = list(np.ndindex(self.shape))
        [self.states.remove(wall) for wall in self.wall_indexlist]
        self.actions = [(-1, 0), (1, 0), (0, 1), (0, -1)]
        self.actionSymbols = dict(zip(self.actions, [self.LEFT, self.RIGHT, self.UP, self.DOWN]))
        self.actionSymbolsRev = dict(zip([self.LEFT, self.RIGHT, self.UP, self.DOWN], self.actions))

        self.policy = {}
        self.value = {}

        for s in gridStates:
            self.value[s] = 0
        
        for s in self.states:
            canActionList = self.supposeAction(s)
            numAction = len(canActionList)
            _isGoal = self.isGoal(s)
            for alist in canActionList:
                a = alist[0]
                self.policy[(s,a)] = 1/numAction if not _isGoal else 0
        

    def supposeAction(self, s):
        canActionList = []
        reward = self.REWARD
        _isGoal = self.isGoal(s)
        for a in self.actions:
            s_new = tuple(np.asarray(s) + np.asarray(a)[::-1])
            canAction = s_new in self.states
            canGoal = s_new == self.S_GOAL
            if canGoal: reward = self.REWARD_NEXTGOAL
            if _isGoal: reward = self.REWARD_GOAL; s_new = self.S_GOAL
            if canAction or canGoal: canActionList.append([a, reward, s_new])

        return canActionList


    def chooseAction(self, s):
        _isGoal = self.isGoal(s)
        canActionList = self.supposeAction(s)
        for alist in canActionList:
            a = alist[0]
            if self.policy[(s, a)] > 0:
                s_new = tuple(np.asarray(s) + np.asarray(a)[::-1])
            if _isGoal: s_new = self.S_GOAL

        return s_new


    def mkPlt(self):
        # make a color map of fixed colors
        self.cmap = colors.ListedColormap(['white', 'yellow', 'grey'])
        bounds=[0.5, 1.5, 2.5, 9.5]
        self.norm = colors.BoundaryNorm(bounds, self.cmap.N)

        self.fig, self.axes = plt.subplots(1,2, figsize=(15, 5))

    def mkMaze(self):
        self.maze = np.zeros(self.shape, dtype=int)

        # Mark location
        self.maze[self.S_START] = self.V_START
        self.maze[self.S_GOAL] = self.V_GOAL

        # Mark wall for maze
        self.maze[2:5,2] = self.V_WALL
        self.maze[1,5] = self.V_WALL
        self.maze[3:6,7] = self.V_WALL

        self.wall_index = np.where(self.maze == self.V_WALL)
        self.wall_indexlist = list(zip(self.wall_index[0], self.wall_index[1]))


    def isWall(self, s):
        if self.maze[s] == self.V_WALL:
            return True
        else:
            return False


    def isGoal(self, s):
        if self.maze[s] == self.V_GOAL:
            return True
        else:
            return False 

    def setMaze(self, ax):
        argMark = dict(fontsize=self.FONTSIZE, ha='center', va='center')

        ax.set_xticks([])
        ax.set_yticks([])
        ax.axis('on')
        ax.spines['left'].set_linewidth(0.5)
        ax.spines['right'].set_linewidth(0.5)
        ax.spines['top'].set_linewidth(0.5)
        ax.spines['bottom'].set_linewidth(0.5)
        ax.spines['left'].set_color('k')
        ax.spines['right'].set_color('k')
        ax.spines['top'].set_color('k')
        ax.spines['bottom'].set_color('k')

        ax.pcolor(self.maze,snap='true', edgecolors='k', linewidths=0.5, cmap=self.cmap, norm=self.norm)

        ax.text(self.S_START[1] + 0.5, self.S_START[0] + 0.5, self.CURRENT, **argMark, color = 'red')
        ax.text(self.S_START[1] + 0.5, self.S_START[0] + 0.5, self.START, **argMark)
        ax.text(self.S_GOAL[1] + 0.5, self.S_GOAL[0] + 0.5, self.GOAL, **argMark)

        plt.close()


    def setAction(self, ax):
        argActionLeft = dict(fontsize=self.FONTSIZE, ha='right', va='center', color='deepskyblue')
        argActionRight = dict(fontsize=self.FONTSIZE, ha='left', va='center', color='deepskyblue')
        argActionUp = dict(fontsize=self.FONTSIZE, ha='center', va='bottom', color='deepskyblue')
        argActionDown = dict(fontsize=self.FONTSIZE, ha='center', va='top', color='deepskyblue')
        argAction = dict(zip(self.actions, [argActionLeft, argActionRight, argActionUp, argActionDown]))

        for s in self.states:
            if self.isWall(s): continue
            if self.isGoal(s): continue
            j = s[0] + 0.5
            i = s[1] + 0.5
            for a in self.actions:
                try:
                    policy = self.policy[(s, a)]
                except KeyError:
                    continue
                if policy <= 0: continue
                actionSymbol = self.actionSymbols[a]
                arg = argAction[a]
                ax.text(i, j, actionSymbol, **arg)

        plt.close()

    def setValue(self, ax):
        value = np.zeros(self.shape)
        for s in self.value.keys():
            value[s] = self.value[s]

        pc = ax.pcolor(value, snap='true', edgecolors='w', linewidths=0.5)
        pc.update_scalarmappable()
        for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):
            x, y = p.vertices[:-2, :].mean(0)
            red, green, blue = color[:3]*255
            brightness = (red*0.299 + green*0.587 + blue*0.114) > 180 #186
            color = 'k' if brightness else  'w'
            ax.text(x, y, str(round(value, 2)), ha="center", va="center", color=color)

        plt.close()

    def draw(self):
        [ax.cla() for ax in self.axes]
        [ax.set_visible(True) for ax in self.axes] 
        self.mkPlt()
        self.countDraw += 1
        self.setMaze(self.axes[0])
        self.setAction(self.axes[0])
        self.setValue(self.axes[1]) 
        # display(self.fig)
        plt.show()

    def getNumActions(self):
        numActions = 0
        curState=self.S_START
        while True:
            numActions += 1
            nextState = self.chooseAction(curState)
            curState = nextState
            if curState == self.S_GOAL: return numActions

    def animate(self):
        self.fig_ani, self.ax_ani = plt.subplots(1,figsize=(7.5, 5)) 
        self.setMaze(self.ax_ani)
        self.setAction(self.ax_ani)

        for t in self.ax_ani.texts:
            if t.get_text() == self.CURRENT:
                self.jerry = t

        def setAnimate(i):
            if i == 0: 
                curState = self.S_START
                curPos = tuple(np.add(curState, (0.5, 0.5))[::-1])
                self.jerry.set_position(curPos)
                return self.ax_ani,
            else:
                curPos = self.jerry.get_position()
                curState = tuple(np.subtract(curPos, (0.5, 0.5))[::-1])
                curPos = self.jerry.get_position()
                curState = tuple(np.subtract(curPos, (0.5, 0.5))[::-1])
                curState_int = tuple(int(x) for x in curState)
                nextState = self.chooseAction(curState_int)
                nextPos = tuple(np.add(nextState, (0.5, 0.5))[::-1])
                self.jerry.set_position(nextPos)
                return self.ax_ani,

        frames = self.getNumActions() + 1
        self.anim = animation.FuncAnimation(self.fig_ani, setAnimate,  
                                            frames=frames, interval=250, blit=False)
        writer = animation.writers['ffmpeg'](fps=30)
        self.anim.save('maze-optimal-path_dynamic-programmingdemo.mp4',writer=writer,dpi=100)


def policyEvaluate(maze, reward_decline=0.9, convergence_thold=1e-10):
  while True:
        error = 0
        for s in maze.states:
            v_old = maze.value[s]
            v_new = 0
            canActionList = maze.supposeAction(s)
            for alist in canActionList:
                a, reward, s_new = alist
                v_new += maze.policy[(s,a)] * (reward + reward_decline * maze.value[s_new])
            maze.value[s] = v_new
            error = max(error ,abs(v_new - v_old))

        if error < convergence_thold:
            break

def policyImprovement(maze, reward_decline=0.9):
    policyStable = True
    for s in maze.states:
        q = {}
        policy_old = {}
        canActionList = maze.supposeAction(s)
        for alist in canActionList:
            a, reward, s_new = alist
            a_old = a
            policy_old[a] = maze.policy[(s, a)]
            q[a] = reward_decline * maze.value[s_new]
        qmax_value = max(q.values())

        # qmax = {key: value for key, value in q.items() if value == qmax_value}
        q_binary = {key: 1 if value == qmax_value else 0 for key, value in q.items() }
        policy_new = {key: value / sum(q_binary.values()) for key, value in q_binary.items()}

        for a in policy_new:
            maze.policy[(s, a)] = policy_new[a]
            if policy_old[a] != policy_new[a]: policyStable = False
    
    return policyStable

def policyIterator(maze):
    while True:
        policyStable = False
        policyEvaluate(maze)
        policyStable = policyImprovement(maze)
        if policyStable:
            break

maze = Maze()
maze.draw()

policyIterator(maze)
maze.draw()

maze.animate()
# Note: below is the part which makes it work on Colab
# rc('animation', html='jshtml')
# maze.anim
